{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupyter Notebook for scraping then analyzing NPS thesis abstracts using ChatGPT 3.5\n",
    "\n",
    "Code was constructed to get thesis abstracts from Naval Postgraduate School (NPS) site and classify them into categories based on the content of their abstracts into specific categories. We ended up first having a browser based ChatGPT 4o build 10 separate and distinct categories to split the data into and define them strictly enough to encourage our prompt to not misclassify. This was dont iteratively and 10 categories was arbitrary. This method is hopefully scalable to other categorizations such as critical technology areas as defined by OUSD (R&E) or by Joint Warfighting Functions. This will hopefully be a reusable method to ensure abstracts can be represented in the aggregate to outside stakeholders by their topic. Currently, only anecdotal research topics are shared by NPS, which could benefit from displaying topic areas cumulatively to give stakeholders a better sense of the defense focused education and impact the theses have on advancing research in relevant areas. \n",
    "\n",
    "## Step 1: Build scrape method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def scrape(int_page, int_size, int_year):\n",
    "    # Define the URL and query parameters\n",
    "    url = \"https://calhoun.nps.edu/server/api/discover/browses/dateissued/items\"\n",
    "    params = {\n",
    "        \"scope\": \"\",\n",
    "        \"sort\": \"default,ASC\",\n",
    "        \"page\": int_page,\n",
    "        \"size\": int_size,\n",
    "        \"startsWith\": int_year,\n",
    "        \"embed\": \"thumbnail\"\n",
    "    }\n",
    "\n",
    "    # Define the headers\n",
    "    headers = {\n",
    "        \"Accept\": \"application/json, text/plain, */*\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br, zstd\",\n",
    "        \"Accept-Language\": \"en;q=1,en-US;q=0.1,enq=0.09\",\n",
    "        \"Content-Type\": \"application/json; charset=utf-8\",\n",
    "        \"Cookie\": \"DSpace-XSRF-COOKIE=; other-cookies-here\",\n",
    "        \"Referer\": \"https://calhoun.nps.edu/browse/dateissued?scope=\",\n",
    "        \"Sec-Ch-Ua\": \"\\\"Chromium\\\";v=\\\"124\\\", \\\"Google Chrome\\\";v=\\\"124\\\", \\\"Not-A.Brand\\\";v=\\\"99\\\"\",\n",
    "        \"Sec-Ch-Ua-Mobile\": \"?0\",\n",
    "        \"Sec-Ch-Ua-Platform\": \"\\\"Windows\\\"\",\n",
    "        \"Sec-Fetch-Dest\": \"empty\",\n",
    "        \"Sec-Fetch-Mode\": \"cors\",\n",
    "        \"Sec-Fetch-Site\": \"same-origin\",\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36\",\n",
    "        \"X-Xsrftoken\": \"\"\n",
    "    }\n",
    "\n",
    "\n",
    "    # Make the API call\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the JSON data\n",
    "        data = response.json()\n",
    "    else:\n",
    "        print(f\"Request failed with status code {response.status_code}\")\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Build JSON parsing method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_data_headers = ['dc.description.abstract',\n",
    "                     'dc.contributor.author',\n",
    "                     'dc.contributor.department',\n",
    "                     'dc.date.issued',\n",
    "                     'dc.description.service',\n",
    "                     'dc.identifier.curriculumcode',\n",
    "                     'dc.title',\n",
    "                     'dc.type',\n",
    "                     'etd.thesisdegree.level',\n",
    "                     'etd.thesisdegree.discipline',\n",
    "                     'etd.thesisdegree.name',\n",
    "                     ]\n",
    "def get_data(json_data, list_num_titles):  \n",
    "    dict_results = {}\n",
    "    for num in list_num_titles:\n",
    "        dict_results[num] = {}\n",
    "        for header in list_data_headers:\n",
    "            try:\n",
    "                dict_results[num][header] = json_data['_embedded']['items'][num]['metadata'][header][0]['value']\n",
    "            except (KeyError, IndexError) as e:\n",
    "                dict_results[num][header] = \"unknown\"\n",
    "            #dict_results[num][header] = json_data['_embedded']['searchResult']['_embedded']['objects'][num]['_embedded']['indexableObject']['metadata'][header][0]['value']\n",
    "    return dict_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Call for all abstracts for 2019-2024, collect relevant metadata, and save to CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to thesis_data_2019.csv\n",
      "Data has been written to thesis_data_2020.csv\n",
      "Data has been written to thesis_data_2021.csv\n",
      "Data has been written to thesis_data_2022.csv\n",
      "Data has been written to thesis_data_2023.csv\n",
      "Data has been written to thesis_data_2024.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "for year in [2019, 2020, 2021, 2022, 2023, 2024]:\n",
    "    data = scrape(0, 1000, year)\n",
    "    results = get_data(data, list(range(1000)))\n",
    "    headers = results[0].keys()\n",
    "\n",
    "    # Open a new CSV file for writing with BOM\n",
    "    file_name = f'thesis_data_{year}.csv'\n",
    "    with open(file_name, 'w', newline='', encoding='utf-8-sig') as csvfile:\n",
    "        # Create a CSV writer object\n",
    "        csvwriter = csv.DictWriter(csvfile, fieldnames=headers)\n",
    "        \n",
    "        # Write the header row\n",
    "        csvwriter.writeheader()\n",
    "        \n",
    "        # Write the data rows\n",
    "        for item in results.values():\n",
    "            csvwriter.writerow(item)\n",
    "            \n",
    "\n",
    "    print(f\"Data has been written to {file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: After observing CSVs, do some initial cleaning and consolidate into 1 CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Get the current directory\n",
    "directory = os.getcwd()\n",
    "\n",
    "# Output file\n",
    "output_file = 'thesis_data_2019-2024.csv'\n",
    "\n",
    "# Initialize an empty DataFrame to hold the consolidated data\n",
    "consolidated_df = pd.DataFrame()\n",
    "\n",
    "# Loop through all files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.startswith('thesis_data_') and filename.endswith('.csv'):\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(os.path.join(directory, filename))\n",
    "        \n",
    "        # Filter out rows where the 'abstract' column is 'unknown'\n",
    "        df_filtered = df[df['dc.description.abstract'] != 'unknown']\n",
    "        \n",
    "        # Append the filtered DataFrame to the consolidated DataFrame\n",
    "        consolidated_df = pd.concat([consolidated_df, df_filtered], ignore_index=True)\n",
    "\n",
    "# Write the consolidated DataFrame to a CSV file\n",
    "consolidated_df.to_csv(output_file, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Further cleaning of newly consolidated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = 'thesis_data_2019-2024.csv'\n",
    "output_file = 'thesis_data_2019-2024_fixed.csv'\n",
    "\n",
    "# Read the CSV file and process each row\n",
    "with open(input_file, 'r', newline='', encoding='utf-8-sig') as infile:\n",
    "    reader = csv.reader(infile)\n",
    "    cleaned_data = []\n",
    "    \n",
    "    for row in reader:\n",
    "        cleaned_row = [field.replace('\\n', ' ').replace('\\r', '') for field in row]\n",
    "        cleaned_data.append(cleaned_row)\n",
    "\n",
    "# Write the cleaned data to a new CSV file\n",
    "with open(output_file, 'w', newline='', encoding='utf-8-sig') as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "    writer.writerows(cleaned_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Build method and supporting prompt to have ChatGPT 3.5 categorize all abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "\n",
    "client = openai.OpenAI(api_key = '')\n",
    "\n",
    "# Load the data\n",
    "abstracts_df = pd.read_csv('thesis_data_2019-2024_fixed.csv')\n",
    "\n",
    "# Define critical technology areas\n",
    "critical_technology_areas = [\n",
    "    \"Artificial Intelligence (AI)\",\n",
    "    \"Autonomy\",\n",
    "    \"Quantum Science\",\n",
    "    \"Hypersonics\",\n",
    "    \"Directed Energy\",\n",
    "    \"Microelectronics\",\n",
    "    \"Cybersecurity\",\n",
    "    \"Biotechnology\",\n",
    "    \"Space Technology\",\n",
    "    \"Advanced Materials\",\n",
    "    \"Integrated Network Systems-of-Systems\",\n",
    "    \"Renewable Energy Generation and Storage\",\n",
    "    \"None/Not Clear\"\n",
    "    # Add more areas as needed\n",
    "]\n",
    "innovation_areas = [\n",
    "    \"Naval Engineering\",\n",
    "    \"Combat Systems\",\n",
    "    \"Cyber and Information Systems\",\n",
    "    \"Data Science and Decisions\",\n",
    "    \"Global Security & Strategic Competition\",\n",
    "    \"Defense Systems Management\",\n",
    "    \"Space Technology and Operations\",\n",
    "    \"Maritime Battlespace Environments\",\n",
    "    \"Modeling, Simulation, & Visualization\"\n",
    "    \"C-C5ISRT\",\n",
    "    \"Long Range Fires\",\n",
    "    \"Terminal Defense\",\n",
    "    \"Contested Logistics\",\n",
    "    \"Maritime Domain Awareness\",\n",
    "    \"Artificial Intelligence\",\n",
    "    \"Intelligent Autonomous Systems\",\n",
    "    \"Naval Operational Architecture\",\n",
    "    \"Modeling & Simulation GEMS/LVC\",\n",
    "    \"Energy & Climate Security\",\n",
    "    #\"Public Policy Analysis\",\n",
    "    #\"Emergency Management\",\n",
    "    #\"Recruiting and Retention\",\n",
    "    #\"Civilian Agency Analysis\",\n",
    "    \"Definitely None\",\n",
    "    \"Not Clear\"\n",
    "]\n",
    "\n",
    "research_areas = [\n",
    "    'Cybersecurity and Information Warfare',\n",
    "    'Human Capital and Leadership',\n",
    "    'Operational Strategies and Tactics',\n",
    "    'Defense Technology and Innovation',\n",
    "    'Supply Chain and Logistics',\n",
    "    'National Security and Defense Policy',\n",
    "    'Environmental and Climate Impact',\n",
    "    'Military Health and Safety',\n",
    "    'Intelligence and Surveillance',\n",
    "    'Policy and Governance',\n",
    "]\n",
    "\n",
    "#file_path = 'system_prompt.txt'\n",
    "#file_path = 'prompt_ccr.txt'\n",
    "#file_path = 'prompt_innovation.txt'\n",
    "file_path = 'prompt_my_own.txt'\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    system_prompt = file.read()\n",
    "\n",
    "# Function to classify an abstract using GPT-3.5-turbo\n",
    "def classify_abstract(abstract, areas):\n",
    "    prompt = (\n",
    "        #f\"Classify the following abstract into one or more of these critical technology areas: {', '.join(areas)}. \"\n",
    "        f\"Classify the following abstract into only one of these specific defense-related research categories: {', '.join(areas)}. \"\n",
    "        f\"Provide a classification that best matches the content of the abstract. Provide no other explanation, just the area you think fit the abstract best.\\n\\n\"\n",
    "        f\"Abstract: {abstract}\\n\\nClassification:\"\n",
    "    )\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=50,\n",
    "        temperature=0.3\n",
    "    )\n",
    "    classifications = response.choices[0].message.content\n",
    "    return classifications\n",
    "\n",
    "# Apply the function to each abstract\n",
    "abstracts_df['Classifications'] = abstracts_df['dc.description.abstract'].apply(lambda x: classify_abstract(x, research_areas))\n",
    "\n",
    "# Save the processed data\n",
    "abstracts_df.to_csv('processed_abstracts_3Jun24.csv', index=False, encoding='utf-8-sig')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
